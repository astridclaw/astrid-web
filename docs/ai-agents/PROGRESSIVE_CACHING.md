# Progressive Caching Strategy for AI Orchestrator

**Status**: âœ… Implemented (January 2025)

## Overview

The AI Orchestrator uses **progressive caching** to build up a comprehensive context cache over the course of a workflow, dramatically reducing token costs on subsequent API calls.

## How It Works

### Phase 1: Initial Cache Creation

On the **first API call**, we mark the system prompt for caching:

```typescript
{
  role: 'system',
  content: [
    {
      type: 'text',
      text: systemPrompt,
      cache_control: { type: 'ephemeral' }  // âœ… Cache ~4k tokens
    }
  ]
}
```

**Result**: The initial system prompt (~4,000 tokens) is cached.

### Phase 2: Progressive Context Building

As Claude explores the codebase, we **cache every 3rd tool result**:

```typescript
// Iteration 3, 6, 9, 12...
if (iteration % 3 === 0) {
  toolResultContent[0].cache_control = { type: 'ephemeral' }
  console.log(`ðŸ’¾ [Cache] Marking iteration ${iteration} for caching`)
}
```

**Result**: File contents and tool results accumulate in the cache.

### Phase 3: Cache Reuse

On subsequent calls, Claude **reads from the cache** instead of sending tokens:

```
Iteration 1:  cacheCreation: 4,000  |  cacheRead: 0
Iteration 2:  cacheCreation: 0      |  cacheRead: 4,000
Iteration 3:  cacheCreation: 2,500  |  cacheRead: 4,000  (adds new files)
Iteration 4:  cacheCreation: 0      |  cacheRead: 6,500
...
Iteration 12: cacheCreation: 0      |  cacheRead: 15,000  (massive savings!)
```

## Token Economics

### Without Caching
```
12 iterations Ã— 7,000 tokens/iteration = 84,000 input tokens
Cost: $3.00 per 1M tokens Ã— 0.084M = $0.25
Rate limit risk: High (30k/min limit)
```

### With Progressive Caching
```
Initial cache creation: 4,000 tokens
Additional cache creation (3 iterations): 6,000 tokens
Cache reads (9 iterations): 54,000 tokens (FREE! Doesn't count against limits)

Billable tokens: 10,000 input tokens
Cost: $3.00 per 1M tokens Ã— 0.010M = $0.03
Rate limit risk: None (cache reads don't count)
```

**Savings**: 88% reduction in cost and rate limit exposure!

## Configuration

### Current Settings

- **Max iterations**: 12 (up from 8)
- **Cache frequency**: Every 3rd iteration
- **Cache type**: Ephemeral (5 min TTL)

### Tuning Considerations

**Cache more frequently (every 2nd iteration):**
- âœ… Faster cache building
- âŒ Higher initial cache creation cost

**Cache less frequently (every 4th iteration):**
- âœ… Lower cache creation cost
- âŒ Slower cache accumulation

**Current strategy (every 3rd)** balances both concerns well.

## Monitoring Cache Usage

### Real-time Monitoring

```bash
# Watch cache activity in real-time
./scripts/monitor-cache-usage.sh
```

### Test Progressive Caching

```bash
# Easy way: Use npm script (loads .env.local automatically)
npm run test:caching

# Or manually with DATABASE_URL
DATABASE_URL="postgresql://postgres:password@localhost:5432/astrid_dev" \
npx tsx scripts/test-progressive-caching.ts
```

### Log Analysis

Look for these log entries:

```
ðŸ’¾ [Cache] Marking iteration 3 for caching
ðŸ“Š [Claude] Token usage - iteration 3 - cacheCreation: 2,459, cacheRead: 4,126
âœ¨ Cache read: 4,126 tokens (FREE!)
```

## Implementation Details

### System Prompt Caching

```typescript
const systemPrompt = this.buildSystemPrompt()

messages.push({
  role: 'system',
  content: [
    {
      type: 'text',
      text: systemPrompt,
      cache_control: { type: 'ephemeral' }
    }
  ]
})
```

### Tool Result Caching

```typescript
const toolResultContent: any[] = [
  {
    type: 'tool_result',
    tool_use_id: toolUseBlock.id,
    content: JSON.stringify(toolResult)
  }
]

// Mark for caching every 3rd iteration
if (iteration % 3 === 0) {
  toolResultContent[0].cache_control = { type: 'ephemeral' }
}

messages.push({
  role: 'user',
  content: toolResultContent
})
```

## Cache Lifecycle

### Ephemeral Cache TTL

- **Duration**: 5 minutes
- **Scope**: Per conversation thread
- **Persistence**: In-memory only

### Cache Invalidation

Cache is automatically cleared when:
- 5 minutes elapse since last use
- Conversation thread ends
- Different repository is accessed
- Model switches (Claude vs OpenAI)

## Expected Behavior

### Successful Caching

```
Iteration 1:  cacheCreation: 4,126  |  cacheRead: 0         |  totalInput: 4,843
Iteration 2:  cacheCreation: 0      |  cacheRead: 4,126     |  totalInput: 2,517
Iteration 3:  cacheCreation: 2,459  |  cacheRead: 4,126     |  totalInput: 7,284
Iteration 4:  cacheCreation: 0      |  cacheRead: 6,585     |  totalInput: 1,892
```

âœ… **Signs of healthy caching:**
- `cacheRead` increases over iterations
- `cacheCreation` only on iterations 1, 3, 6, 9, 12
- `totalInput` stays under 10k per iteration

### Problematic Caching

```
Iteration 1:  cacheCreation: 0      |  cacheRead: 0         |  totalInput: 15,843
Iteration 2:  cacheCreation: 0      |  cacheRead: 0         |  totalInput: 18,517
```

âŒ **Signs of cache issues:**
- `cacheRead` always 0
- `cacheCreation` always 0
- `totalInput` consistently high (>15k)

**Troubleshooting**: Check that `cache_control` markers are present in messages.

## Future Enhancements

### Persistent Caching (Future)

Anthropic is working on **persistent caching** that survives across conversations:

```typescript
cache_control: { type: 'persistent', ttl: 86400 }  // 24 hours
```

This would allow:
- âœ… Codebase context cached for entire day
- âœ… Zero cache creation cost after first workflow
- âœ… Sub-second planning phase (cache already warm)

### Adaptive Cache Strategy

Automatically adjust cache frequency based on:
- File read patterns (cache after large file reads)
- Iteration progress (cache more frequently near end)
- Token usage trends (cache aggressively if approaching limits)

## References

- [Anthropic Prompt Caching Guide](https://docs.anthropic.com/claude/docs/prompt-caching)
- [lib/ai-orchestrator.ts](../../lib/ai-orchestrator.ts) - Implementation
- [scripts/test-progressive-caching.ts](../../scripts/test-progressive-caching.ts) - Test script
- [scripts/monitor-cache-usage.sh](../../scripts/monitor-cache-usage.sh) - Monitoring tool

---

**Last Updated**: January 2025
**Implemented By**: Claude Code
**Status**: Production-ready âœ…
