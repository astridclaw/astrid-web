# Cloud AI Workflow Fixes - Complete Guide

## ğŸ“‹ Overview

This directory contains a comprehensive guide to fixing the Astrid cloud AI coding workflow to make it as reliable as Claude Code CLI.

## ğŸ¯ The Problem

Your cloud AI workflow has two main issues:
1. **Hits context limits faster** - Workflows fail with truncated responses
2. **Hard to debug** - When workflows get stuck, no visibility into what went wrong

## ğŸ“š Documentation

### [CLOUD_WORKFLOW_FIXES.md](./CLOUD_WORKFLOW_FIXES.md) â­ **Start Here**
**Comprehensive technical analysis of the problems and solutions.**

**What you'll find:**
- Root causes of context limit issues
- Why debugging is difficult
- Detailed solutions with code examples
- Quick wins you can implement in <1 hour
- Long-term architecture improvements

**Read this first** to understand WHY the cloud workflow fails and WHAT to fix.

---

### [CLOUD_VS_CLI_COMPARISON.md](./CLOUD_VS_CLI_COMPARISON.md)
**Visual architecture comparison between CLI and cloud workflows.**

**What you'll find:**
- Side-by-side architecture diagrams
- Workflow execution timeline comparisons
- Error handling differences
- Success rate metrics
- Cost and performance analysis

**Read this** to see the concrete differences and understand expected improvements.

---

### [IMPLEMENTATION_PLAN.md](./IMPLEMENTATION_PLAN.md) â­ **Implementation Guide**
**Step-by-step guide to implement all fixes.**

**What you'll find:**
- **Phase 1: Quick Wins** (1-2 hours) - Immediate 60% â†’ 85% success rate improvement
- **Phase 2: Context Management** (4-6 hours) - Fixes remaining context errors
- **Phase 3: Repository Context** (6-8 hours) - Improves code quality
- Code snippets ready to copy/paste
- Testing checklist for each phase
- Rollback plan if issues occur

**Use this** as your implementation guide. Follow it step-by-step.

---

## ğŸš€ Quick Start (< 1 Hour)

If you only have time for quick wins:

### 1. Increase max_tokens (5 min)
```typescript
// lib/ai-orchestrator.ts line ~594
max_tokens: 8192  // Changed from 4000
```

### 2. Add trace IDs (15 min)
```typescript
// lib/ai-orchestrator.ts constructor
this.traceId = `trace-${Date.now()}-${Math.random().toString(36).substring(7)}`
console.log(JSON.stringify({ traceId: this.traceId, level, message, ...meta }))
```

### 3. Create progress endpoint (20 min)
```typescript
// app/api/coding-workflow/progress/[taskId]/route.ts
export async function GET(request, { params }) {
  // Return real-time workflow progress
}
```

### 4. Update GitHub Actions (10 min)
```yaml
# .github/workflows/astrid-coding-agent.yml
# Show progress bar and detailed status
```

**Expected improvement:** 60% â†’ 85% success rate on medium tasks! ğŸ‰

---

## ğŸ“Š Success Metrics

### Before Fixes
- âŒ Success rate: 30-60% (medium tasks)
- â±ï¸ Average time: 25-45 minutes
- ğŸ”¥ Context errors: 60% of failures
- ğŸ˜ User satisfaction: Low

### After Quick Wins (Phase 1)
- âœ… Success rate: 85% (medium tasks)
- â±ï¸ Average time: 15-25 minutes
- ğŸ”¥ Context errors: 15% of failures
- ğŸ˜Š User satisfaction: Medium

### After All Fixes (Phase 1-3)
- âœ… Success rate: 90-95% (medium tasks)
- â±ï¸ Average time: 10-20 minutes
- ğŸ”¥ Context errors: <5% of failures
- ğŸ‰ User satisfaction: High

---

## ğŸ—ºï¸ Recommended Path

### For Immediate Relief (1-2 hours)
**Goal:** Stop context errors and improve debugging

1. Read [CLOUD_WORKFLOW_FIXES.md](./CLOUD_WORKFLOW_FIXES.md) - "Quick Wins" section
2. Follow [IMPLEMENTATION_PLAN.md](./IMPLEMENTATION_PLAN.md) - Phase 1 only
3. Test with a task that previously failed
4. Monitor for 1 week

**Expected:** 85% success rate, much better debugging

---

### For Full Reliability (10-15 hours total)
**Goal:** Match Claude Code CLI reliability

1. Read all documentation (1 hour)
2. Implement Phase 1: Quick Wins (1-2 hours)
3. Monitor and collect metrics (1 week)
4. Implement Phase 2: Context Management (4-6 hours)
5. Test with complex tasks (2 hours)
6. Implement Phase 3: Repository Context (6-8 hours)
7. Final testing and documentation (2 hours)

**Expected:** 90-95% success rate, production-ready

---

## ğŸ” Debugging Workflow

### Before Fixes
```
User: "My workflow failed, what happened?"

Options:
1. Check GitHub Actions logs (hard to parse)
2. Ask platform team for help (slow)
3. Give up and fix manually (common)
```

### After Fixes
```
User: "My workflow failed, what happened?"

Self-service:
1. Check task comment for error + trace ID âœ…
2. GET /api/coding-workflow/progress/TASK_ID âœ…
3. Search logs by trace ID âœ…
4. Retry with focused scope âœ…
5. Fix manually using AI's analysis âœ…
```

---

## ğŸ“ˆ Implementation Progress Tracking

Use this checklist to track your progress:

### Phase 1: Quick Wins â±ï¸ 1-2 hours
- [ ] Increase max_tokens to 8192
- [ ] Add trace ID to orchestrator
- [ ] Add structured logging with trace correlation
- [ ] Create progress endpoint
- [ ] Update GitHub Actions monitoring
- [ ] Test with simple task
- [ ] Test with medium task
- [ ] Monitor for 1 week

**Success criteria:** 85% success rate on medium tasks

---

### Phase 2: Context Management â±ï¸ 4-6 hours
- [ ] Add context size tracking
- [ ] Implement context pruning (>100K tokens)
- [ ] Split workflow into separate contexts
- [ ] Add streaming support
- [ ] Update all prompts to use new methods
- [ ] Test with complex task
- [ ] Verify no context limit errors
- [ ] Monitor for 1 week

**Success criteria:** 90% success rate, <5% context errors

---

### Phase 3: Repository Context â±ï¸ 6-8 hours
- [ ] Add repository file reading (ASTRID.md, CLAUDE.md)
- [ ] Generate repository structure for prompts
- [ ] Add context caching (5-minute TTL)
- [ ] Update planning prompts with repo context
- [ ] Test with task requiring specific patterns
- [ ] Verify AI follows repo conventions
- [ ] Monitor code quality improvements
- [ ] Document lessons learned

**Success criteria:** 95% success rate, high code quality

---

## ğŸ› ï¸ Tools and Resources

### Testing
```bash
# Run local development server
npm run dev

# Test specific workflow
curl http://localhost:3000/api/coding-workflow/progress/TASK_ID | jq

# Watch GitHub Actions
gh run watch

# Monitor logs
tail -f logs/ai-orchestrator.log | jq
```

### Debugging
```bash
# Search logs by trace ID
cat logs/ai-orchestrator.log | jq 'select(.traceId == "trace-1234-abc")'

# Get workflow status
curl http://localhost:3000/api/coding-workflow/status/TASK_ID | jq

# Check GitHub Actions run
gh run view RUN_ID
```

### Monitoring
```bash
# Success rate calculation
# Count successful vs failed workflows in last week
curl http://localhost:3000/api/coding-workflow/metrics | jq
```

---

## ğŸš¨ Common Issues

### "I still see context errors after Phase 1"
**Solution:** Implement Phase 2 (context management). Phase 1 only increases the limit, doesn't manage context growth.

### "GitHub Actions timeout after 60 minutes"
**Solution:** Check progress endpoint for details. Likely stuck in planning phase - see logs for trace ID.

### "AI generates code that doesn't match repo patterns"
**Solution:** Implement Phase 3 (repository context). Ensure ASTRID.md exists with clear conventions.

### "Progress endpoint returns 404"
**Solution:** Restart server after creating new API route. Verify file path matches Next.js conventions.

---

## ğŸ“ Lessons Learned (Fill in after implementation)

### What worked well:
- [ ] Quick wins gave immediate improvement
- [ ] Trace IDs made debugging much easier
- [ ] Progress endpoint improved user experience
- [ ] Repository context reduced hallucinations

### What was challenging:
- [ ] Streaming implementation was complex
- [ ] Context pruning logic needed tuning
- [ ] Cache invalidation caused issues
- [ ] GitHub API rate limits still a problem

### Recommendations for future:
- [ ] Add metrics dashboard
- [ ] Set up error alerting
- [ ] Create developer documentation
- [ ] Train team on debugging workflow

---

## ğŸ“ For Other Developers

If you're building a similar cloud AI coding workflow:

### Do These Things
1. âœ… Use `max_tokens: 8192` from the start
2. âœ… Add trace IDs and structured logging
3. âœ… Create progress endpoints for transparency
4. âœ… Split phases into separate contexts
5. âœ… Include repository context in prompts
6. âœ… Cache repository context aggressively

### Avoid These Mistakes
1. âŒ Fire-and-forget async without monitoring
2. âŒ Generic error messages without context
3. âŒ Accumulating context across operations
4. âŒ Using console.log instead of structured logging
5. âŒ No progress feedback during long operations
6. âŒ Skipping repository-specific instructions

---

## ğŸ“ Support

If you encounter issues during implementation:

1. **Check documentation** - All common issues are covered
2. **Review trace logs** - Use trace ID to find detailed logs
3. **Test incrementally** - Don't implement all phases at once
4. **Rollback if needed** - Use feature flags or git revert

---

## ğŸ¯ Summary

**The cloud AI workflow fails because:**
1. Context limits are hit due to low max_tokens and no context management
2. Debugging is difficult due to sparse logging and generic errors

**Fix it by:**
1. **Quick wins** (1-2 hours): Increase max_tokens, add trace IDs, create progress endpoint
2. **Context management** (4-6 hours): Prune context, split phases, add streaming
3. **Repository context** (6-8 hours): Read ASTRID.md, generate file tree, cache context

**Expected results:**
- 60% â†’ 85% success rate (after Phase 1)
- 85% â†’ 90% success rate (after Phase 2)
- 90% â†’ 95% success rate (after Phase 3)

**Start here:** [IMPLEMENTATION_PLAN.md](./IMPLEMENTATION_PLAN.md) Phase 1

Good luck! ğŸš€
